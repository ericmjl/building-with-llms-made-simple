{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"building with llms made simple }}","text":"<p>Welcome to the repository for the building with llms made simple }} project!</p>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#install-from-source","title":"Install from source","text":"<pre><code>pip install git@github.com:ericmjl/building-with-llms-made-simple\n</code></pre>"},{"location":"#build-and-preview-docs","title":"Build and preview docs","text":"<pre><code>mkdocs serve\n</code></pre>"},{"location":"#why-this-project-exists","title":"Why this project exists","text":"<p>Place your reasons here for why this project exists.</p> <p>What benefits does this project give to users?</p>"},{"location":"api/","title":"Top-level API for building with llms made simple","text":"<p>::: building_with_llms_made_simple</p>"},{"location":"tutorial_proposals/scipy_2025/","title":"SciPy 2025 Tutorial Proposal","text":""},{"location":"tutorial_proposals/scipy_2025/#abstract","title":"Abstract","text":"<p>In this tutorial, you will learn how to integrate Large Language Models (LLMs) directly into Python programs as thoughtfully-designed core components of the program rather than bolt-on additions. This hands-on session teaches design principles and practical techniques for incorporating LLM outputs into program control flow. We will use LlamaBot, an open-source Python interface to LLMs, focusing on local execution with local and efficient models.</p>"},{"location":"tutorial_proposals/scipy_2025/#description","title":"Description","text":"<p>This hands-on tutorial teaches practical integration of Large Language Models (LLMs) into Python programs using LlamaBot and Ollama. Working with locally-run models that fit within 16GB RAM, participants will build a git commit message generator while learning core concepts of LLM application development.</p> <p>Through Jupyter notebooks, we'll progress from basic LLM interactions with SimpleBot to structured outputs using Pydantic, culminating in systematic evaluation and practical deployment. The tutorial emphasizes learning-by-doing: participants will experiment with different models, prompting strategies, and temperature settings to understand their effects on output quality.</p> <p>Key learning outcomes include mastering prompt design, implementing structured generation with schema validation, developing systematic evaluation approaches, and integrating LLM-powered features into existing workflows. The session concludes with a class-chosen discussion on broader implications of LLM applications in practice.</p>"},{"location":"tutorial_proposals/scipy_2025/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python programming experience</li> <li>Hardware Requirements:</li> <li>Mac: 16GB RAM minimum</li> <li>Linux/Windows: 16-32GB RAM</li> <li>Ability to install and run Ollama and the <code>smollm2</code>, <code>gemma2:2b</code>, <code>mistral</code>, and <code>phi4</code> models</li> </ul>"},{"location":"tutorial_proposals/scipy_2025/#use-of-ai","title":"Use of AI","text":"<p>AI was used as a typing assistance tool, as I have carpal tunnel. Specifically, this proposal was transcribed from my head using Whisper (speech-to-text) as a first draft. It was then formatted into the SciPy submission format using Claude 3.5 Sonnet. It was edited heavily, ~70% edited, mostly through Whisper transcription.</p>"},{"location":"tutorial_proposals/scipy_2025/#outline","title":"Outline","text":"<p>Part 1: Introduction to LLM APIs with <code>SimpleBot</code> (45 min)</p> <ul> <li>We will use LlamaBot as an API frontend and Ollama as a local LLM provider.</li> <li>Hands-on: Creating a <code>llamabot.SimpleBot</code> to interact with a language model.</li> <li>Brief notes: anatomy of an LLM API call: system prompt, user prompt, temperature, model name.</li> <li>Exercise: \"Hello LLM!\". Change system prompt to set LM persona, change temperature for greater variation in responses, vary user prompts based on user intent</li> <li>Exercise: Guided build of prototype of a git commit message generator. Vary system prompt, temperature, user prompt; vibe check outputs.</li> <li>Exercise: Change LLM model and re-vibe check outputs.</li> <li>Demo: Outputs using <code>gpt-4o</code> v.s. <code>llama3.2</code> + tutorial class evaluation discussion.</li> <li>Key concepts:</li> <li>API calls are state-less/memoryless.</li> <li>Prompts can be designed (not engineered!) to steer the LM to do what we need.</li> <li>Vibe checking is the first thing needed to be done when evaluating an LLM.</li> </ul> <p>Break (15 min)</p> <p>Part 2: Structured Outputs <code>StructuredBot</code> (45 min)</p> <ul> <li>We will introduce the use of <code>StructuredBot</code> to generate outputs that conform to a pre-specified schema.</li> <li>Brief lecture: how structured outputs are generated:</li> <li>Prompting to get JSON.</li> <li>Logits masking. &lt;-- will give us the perfect opportunity to show how most modern LMs work: autoregressive generation.</li> <li>Hands-on: Restructure the git commit message generator from free text to a structured form.</li> <li>Exercise: Decompose a git commit message into its constituent components, implement it as a Pydantic model, use <code>StructuredBot</code> to generate commit message, and format it.</li> <li>Exercise: Add jazz and snazz to the git commit message by adding emojis!</li> <li>Exercise: Add custom class methods to format the commit message.</li> <li>Key concepts:</li> <li>Templated text is a form, model it using Pydantic, and use structured generation methods to fill it in.</li> <li>Content that we require an LLM to generate requires sufficient context to be provided.</li> </ul> <p>Break (15 min)</p> <p>Part 3: Evaluation and baking LLM-based text generation into a product (45 min)</p> <ul> <li>We will introduce the methodology behind evaluating LLM-generated text.</li> <li>Group hands-on: Evaluations</li> <li>Class group exercise: Systematically evaluate the accuracy of the commit message against curated commit messages. Critique where the commit message writer misses information. Identify where additional context needs to be provided, e.g. intents behind changes (usually not available in code).</li> <li>Exercise: Propose and implement changes to the git commit message composer.</li> <li>Hands-on: Baking LLM-based text generation into a product</li> <li>Exercise: Incorporate text generation in a shell executable that automatically composes commit messages.</li> <li>Demo: Show how LlamaBot goes further and hooks directly into git's <code>prepare-commit-msg</code> hook.</li> <li>Key concepts:</li> <li>Systematic evaluation requires effort, the effort put in should be proportional to the impact of the generated text.</li> <li>Operationalizing LLM text generation requires thoughtfulness in workflow integration.</li> </ul> <p>Break (15 min)</p> <p>Part 4: Discussion of class' choice (20 min)</p> <ul> <li>Four choices:</li> <li>Option 1: Design principles for LLM-enhanced applications.</li> <li>Option 2: Ideation brainstorm for the application of LLMs to accelerate one's work.</li> <li>Option 3: What's the future of data science roles in GenAI?</li> <li>Option 4: Ethics discussion.</li> <li>Class will choose by voting. Most popular wins.</li> </ul>"},{"location":"tutorial_proposals/scipy_2025/#additional-information","title":"Additional Information","text":"<p>Content is being developed on a branch of this repo: Building with LLMs Made Simple.</p>"},{"location":"tutorial_proposals/scipy_2025/#notes","title":"Notes","text":"<p>I have extensive experience teaching tutorials at SciPy, including topics on Network Analysis, Bayesian Statistics, and Deep Learning Fundamentals. The format of this tutorial follows best practices that I have learned from previous tutorials.</p>"}]}