# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "llamabot[all]==0.12.11",
#     "marimo",
#     "pyprojroot==0.3.0",
#     "rich==14.0.0",
#     "lancedb",
#     "sentence-transformers",
#     "chonkie==1.0.10",
#     "building-with-llms-made-simple==0.0.1",
# ]
#
# [tool.uv.sources]
# building-with-llms-made-simple = { path = "../", editable = true }
# ///

import marimo

__generated_with = "0.14.9"
app = marimo.App(width="medium")


@app.cell
def _():
    import marimo as mo

    return (mo,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    # Part 3: Retrieval Augmented Generation (RAG) with Memory

    In this notebook, we'll explore how to build RAG systems that can maintain
    conversation context. We'll focus on creating a system that can answer questions
    based on a knowledge base while remembering previous interactions.

    ## Learning Objectives

    By the end of this notebook, you will be able to:

    1. Understand how RAG systems combine retrieval with generation
    2. Set up and manage local document stores for knowledge and memory
    3. Create a QueryBot that maintains conversation context through memory
    4. Implement effective text chunking strategies
    """
    )
    return


@app.cell
def _():
    from pathlib import Path

    import llamabot as lmb
    from llamabot.components.docstore import LanceDBDocStore
    from rich import print

    return LanceDBDocStore, lmb, print


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## RAG in 5 minutes

    <iframe src="https://link.excalidraw.com/readonly/Hd9NUurFW5YdM0zYxUwZ" width="100%" height="800" style="border: none;"></iframe>
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## Setting Up Document Stores

    To build our RAG system, we'll need a knowledge base for our documents.
    We'll use LanceDB, a lightweight vector database that, most crucially,
    comes with hybrid search capabilities.
    Within LlamaBot, we provide a `LanceDBDocStore` that has a uniform interface
    and sane defaults on LanceDB that should "just work" for a variety of applications.
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### Set up a LanceDB document store

    In the cell below, I have created for you two LanceDBDocStores,
    one for document storage about ZenThing,
    and the other for storing conversation memories.
    The API for the docstore is:

    ```python
    import llamabot as lmb

    # Replace table_name with a human-readable name for what you're storing in here.
    ds = lmb.LanceDBDocStore(table_name="some informative name here")
    ```

    If you're curious to see how `create_knowledge_store` and `create_memory_store` are implemented,
    feel free to peek inside the repo.
    """
    )
    return


@app.cell
def _():
    # Create the document stores
    from building_with_llms_made_simple.answers.rag_answers import (
        create_knowledge_store,
        create_memory_store,
        create_rag_bot,
    )

    # Comment the following lines and try to write the code yourself!
    knowledge_store = create_knowledge_store()
    memory_store = create_memory_store()

    # Your answers here!
    return create_rag_bot, knowledge_store, memory_store


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## What's going on here?

    1. We are intializing two empty LanceDB databases, one for chat memory purposes, and one for document storage purposes.
    2. Underneath the hood, we are setting our vector dims for document embeddings to be 128 in length.
    3. LanceDBDocStores are configured to automatically use **hybrid search**, which means we do both keyword and vector search, so the ColBERTReranker is used to combine the results from both searches into a single list.
    4. The embedding model used in Llamabot is the `minishlab/potion` model, which is 100-500X faster than sentence-transformers and "good enough" for most purposes.
    5. You can ignore the term "Linear dim" ðŸ¤—.
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## Creating Sample Documents

    Let's use some sample documents about a fictional programming language
    called Zenthing. We'll use these to demonstrate how RAG works with a
    knowledge base.

    ### Detour: Why Zenthing?

    We're using a fictional programming language
    to ensure the bot relies solely on our provided knowledge.
    In doing so, we can attempt to avoid confusion with real programming
    languages.
    This is a good way to test the bot's ability to retrieve information
    from a knowledge base and not from its own background knowledge.
    """
    )
    return


@app.cell
def _(knowledge_store):
    # Sample documents about a new programming language named Zenthing
    from building_with_llms_made_simple.answers.rag_answers import (
        zenthing_ecosystem_use_cases_text,
        zenthing_language_features_text,
        zenthing_overview_text,
        zenthing_syntax_and_programming_style_text,
    )

    zenthing_docs = [
        zenthing_ecosystem_use_cases_text,
        zenthing_language_features_text,
        zenthing_overview_text,
        zenthing_syntax_and_programming_style_text,
    ]

    # Add documents to knowledge store
    knowledge_store.extend(zenthing_docs)  # Using extend for bulk addition
    return (zenthing_docs,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""One of the first things I would like to disambiguate here is that documents are nothing more than text! Given the current state of technology, vector stores most commonly will accept plain text, images, and audio. Complex documents such as PDFs and word documents need to be converted into plain text first."""
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    Now, let's try retrieving documents from the docstore.
    The way that this generically works is as follows:

    ```python
    # replace "docstore", and "query goes here..." and `n_results`.
    docstore.retrieve("query goes here...", n_results=2)
    ```

    Try out different queries for yourself.
    """
    )
    return


@app.cell
def _(knowledge_store):
    # Retrieve something from the document store!
    knowledge_store.retrieve("how does zenthing work?", n_results=2)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""The retrieved documents can now be passed to an LLM in combination with the user query and system prompt to generate the answer."""
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### Exercise: Create a QueryBot

    Now, let's create a QueryBot that can use both our knowledge base and
    conversation history.
    We'll configure it to:
    (a) find relevant information in our documents (using a LanceDBDocStore), and
    (b) remember previous conversations through a memory module.

    The way to create a QueryBot is as follows:


    ```python
    qb = lmb.QueryBot(
        system_prompt="You are an expert in the Zenthing programming language.",
        docstore=..., # replace with your knowledge store object object
        memory=..., # replace with your memory store object
        model_name="provider/model_name", # e.g. ollama_chat/llama3.2
        # other SimpleBot kwargs go here.
    )
    ```
    """
    )
    return


@app.cell
def _(create_rag_bot, knowledge_store, memory_store):
    # Comment out the following line if you're going to implement QueryBot on your own.
    rag_bot = create_rag_bot(knowledge_store, memory_store)

    # Create the QueryBot
    return (rag_bot,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### Exercise: Test the RAG Bot

    Let's test our RAG bot with a series of questions about Zenthing.
    We'll observe how it:

    1. Uses the knowledge base to provide accurate information
    2. Maintains context from previous questions
    3. Combines both sources for comprehensive answers

    Recall that the way to make queries of a LlamaBot bot is:

    ```python
    response = querybot("query goes here...")
    ```
    """
    )
    return


@app.cell
def _():
    # Try asking the bot about what is Zenthing.
    return


@app.cell
def _():
    # Try asking about Zenthing's key features.
    return


@app.cell
def _():
    # Try asking how Zenthing is used in Data Science.
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""## Demo: using Marimo UI elements to quickly stand up a prototype""")
    return


@app.cell
def _(mo, rag_bot):
    def chat_callback(messages, config):
        # Each message has a `content` attribute, as well as a `role`
        # attribute ("user", "system", "assistant");
        question = messages[-1].content
        return rag_bot(question).content

    chat = mo.ui.chat(chat_callback)
    chat
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## Advanced Text Chunking Strategies

    We're now going to talk about chunking strategies.

    Chunking is a critical component of RAG systems that determines
    how effectively the system can retrieve and use information.
    In this section,
    we'll explore different chunking strategies and their applications.
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### Why chunking matters

    Language models have limited context windows,
    which means you can't guarantee that you can process entire documents at once.
    Even the 1M context windows can't stuff all of your company's documents together.
    And moreover, LLMs are known to suffer from the needle-in-a-haystack problem
    when given too much information to answer a very specific request.

    Chunking helps by breaking documents into manageable pieces.
    If done right, you can preserve semantic meaning within chunks,
    but if not done right, you might break documents in undesirable places.
    As such, it's important to figure out a sane chunking strategy,
    based on the kinds of documents that you encounter.

    In the Zenthing example above, we dispensed with chunking and simply stored each document on its own.
    In the following section, we'll see what it's like to chunk documents using different strategies,
    and explore whether they will have an effect on the generated output text.
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## Introducing Chonkie: A Modern Text Chunking Library

    Chonkie is a powerful Python library designed specifically for text chunking in RAG applications.
    It provides several advantages:

    1. **Multiple Chunking Strategies**: From simple token-based to sophisticated recursive chunking
    2. **Customizable Parameters**: Fine-tune chunk sizes, overlaps, and boundaries
    3. **Language Support**: Built-in support for multiple languages and document types
    4. **Easy Integration**: Simple API that works well with popular vector stores

    In this section, we'll explore different chunking strategies using Chonkie
    and see how they affect our RAG system's performance.
    """
    )
    return


@app.cell
def _(zenthing_docs):
    from chonkie import TokenChunker

    text_to_chunk = "\n".join(zenthing_docs)

    # Basic initialization with default parameters
    chunker_basic = TokenChunker(
        tokenizer="gpt2",  # Supports string identifiers
        chunk_size=128,  # Maximum tokens per chunk
        chunk_overlap=8,  # Overlap between chunks
    )

    chunks_basic = chunker_basic(text_to_chunk)
    chunks_basic
    return chunks_basic, text_to_chunk


@app.cell
def _(chunks_basic, lmb):
    zenthing_basic_chunks_docstore = lmb.LanceDBDocStore(
        table_name="zenthing_basic_chunks_docstore"
    )
    zenthing_basic_chunks_docstore.reset()
    zenthing_basic_chunks_docstore.extend([chunk.text for chunk in chunks_basic])
    return (zenthing_basic_chunks_docstore,)


@app.cell
def _(mo):
    mo.md(
        r"""**Note:** The settings above are by no means sane defaults, they were tuned to this tutorial to make some points more evident!"""
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### Discussion

    What are salient properties that you notice about the chunks? Specifically, where do the chunks start and end?
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### Sentence Chunking

    Let's try a different chunker from Chonkie, the `SentenceChunker`, and let's see how the chunks look different.
    """
    )
    return


@app.cell
def _(text_to_chunk):
    from chonkie import SentenceChunker

    # Basic initialization with default parameters
    chunker_sentence = SentenceChunker(
        tokenizer_or_token_counter="gpt2",  # Supports string identifiers
        chunk_size=128,  # Maximum tokens per chunk
        chunk_overlap=8,  # Overlap between chunks
        min_sentences_per_chunk=1,  # Minimum sentences in each chunk
    )
    chunks_sentence = chunker_sentence(text_to_chunk)
    chunks_sentence
    return (chunks_sentence,)


@app.cell
def _(chunks_sentence, lmb):
    zenthing_sentence_chunks_docstore = lmb.LanceDBDocStore(
        table_name="zenthing_sentence_chunks_docstore"
    )
    zenthing_sentence_chunks_docstore.reset()
    zenthing_sentence_chunks_docstore.extend([chunk.text for chunk in chunks_sentence])
    return (zenthing_sentence_chunks_docstore,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### Discussion

    Compare the chunks from the `SentenceChunker` and the `TokenChunker`. How do they differ?
    """
    )
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ## How does chunking change answers?

    We are going to measure this by comparing answers with a QueryBot that has access to each of the two docstores. This is going to be pretty trippy with Bot surgery and monkey-patching, I hope you enjoy the ride ðŸ™ƒ.
    """
    )
    return


@app.cell
def _(mo, rag_bot):
    # First off, lobotomy -- disable rag_bot's chat_memory.
    rag_bot.memory = None

    question = mo.ui.text_area(label="Question about Zenthing")
    question
    return (question,)


@app.cell
def _(
    mo,
    question,
    rag_bot,
    zenthing_basic_chunks_docstore,
    zenthing_sentence_chunks_docstore,
):
    # Next up, swap docstores on-the-fly.
    rag_bot.docstore = zenthing_basic_chunks_docstore
    response_basic = rag_bot(question.value)
    basic_md = mo.md(response_basic.content)

    # Next up, swap docstores again
    rag_bot.docstore = zenthing_sentence_chunks_docstore
    response_sentence = rag_bot(question.value)
    sentence_md = mo.md(response_sentence.content)

    # Show the two responses side-by-side.
    response_md = mo.hstack([basic_md, sentence_md])
    response_md
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## Discussion: What's the difference?

    Is there a difference between the two ways of chunking, for the given query?

    - Sometimes this will be query-dependent.
    - Can you think of a situation where the two response will look different?
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## Custom Chunking

    Some special kinds of documents may need a different chunking and processing strategy
    before being stored in a DocStore.

    For example, if you want to enable searching through laboratory protocols
    with the goal of guiding people to a very specific section,
    you may want to chunk by section instead to make citations easy.

    Here, I am going to provide you with three texts:

    1. A `lab_protocol_text`,
    2. A `cleaning_protocol_text`, and
    3. A `quality_control_protocol_text`.
    """
    )
    return


@app.cell
def _(print):
    from building_with_llms_made_simple.answers.rag_answers import (
        lab_protocol_text,
        cleaning_protocol_text,
        quality_control_protocol_text,
    )

    # Preview the texts before proceeding
    print(lab_protocol_text)
    print("-----")
    print(cleaning_protocol_text)
    print("-----")
    print(quality_control_protocol_text)
    return (
        cleaning_protocol_text,
        lab_protocol_text,
        quality_control_protocol_text,
    )


@app.cell
def _(mo):
    mo.md(
        r"""
    ### Discussion

    How would you choose to chunk these documents? Discuss this with your neighbor.

    What other chunking strategies would be handy here?

    Then let's share what we've discussed in the larger group.
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### Demo

    As a demo, I am showing below how to chunk a document with a custom chunker (not using Chonkie now). The goal is to get each top-level section into a single chunk.
    """
    )
    return


@app.cell
def _(
    cleaning_protocol_text,
    lab_protocol_text,
    quality_control_protocol_text,
):
    from building_with_llms_made_simple.answers.rag_answers import insert_delimiter

    sop_chunks = []
    titles_to_texts = {
        "lab protocol": lab_protocol_text,
        "cleaning protocol": cleaning_protocol_text,
        "quality control protocol": quality_control_protocol_text,
    }
    for title, text in titles_to_texts.items():
        preprocessed = insert_delimiter(text, level=1)
        txt_chunks = preprocessed.split("|||SECTION|||")
        for txt in txt_chunks:
            txt += f"\n(from document {title})"
            sop_chunks.append(txt)
    sop_chunks
    return (sop_chunks,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""Because we have 3 documents mixed together, one design choice I have made is to append the document source to the end of the chunk so that it maintains its connection to the original document title. We can discuss pros/cons about this later."""
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## Demo: Build another QueryBot

    Now, let's build an SOP bot and try to query it. Key points for this demo:

    1. Create an `sop_docstore`, and an `sop_memorystore`, both being `LanceDBDocStore`s.
    2. Add the chunks that we just
    3. Create an `sop_bot`, passing in `sop_docstore` and `sop_memorystore` into the `QueryBot` constructor.
    4. Then, query the SOP bot!
    """
    )
    return


@app.cell
def _(LanceDBDocStore, lmb, sop_chunks):
    from building_with_llms_made_simple.answers.rag_answers import (
        rag_bot_sysprompt,
    )

    sop_docstore = LanceDBDocStore(
        table_name="sop_docstore",
    )
    # For this experiment, clear out the docstore just in case we are re-running stuff.
    sop_docstore.reset()
    sop_docstore.extend(sop_chunks)

    sop_memorystore = LanceDBDocStore(table_name="sop_bot_memory")
    sop_memorystore.reset()

    sop_bot = lmb.QueryBot(
        system_prompt=rag_bot_sysprompt(),
        docstore=sop_docstore,
        memory=sop_memorystore,
        model_name="ollama_chat/llama3.2",
        temperature=0.0,  # Keep responses minimally stochastic
    )
    return (sop_bot,)


@app.cell
def _(sop_bot):
    sop_bot("How do we measure the quality of our samples?")
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""We'll now try another query.""")
    return


@app.cell
def _(sop_bot):
    response = sop_bot(
        "What are the roles of the personnel involved in quality monitoring?"
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## Take-Home Exercise: Build a ChatUI for SOPBot

    Using what you've seen above, make a ChatUI for the SOPBot below.
    """
    )
    return


@app.cell
def _():
    # Your code here!
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### Take-Home Exercise: ðŸ”´-team the bot

    Your task: try your best to find cases where the LLM fails to answer a relevant question correctly!
    """
    )
    return


@app.cell
def _():
    # Your code here!
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## Discussion.

    SOPs are a single example of the kinds of documents that we would process. I synthesized the examples we have here based on discussions with friends and colleagues across the industry that I work in.

    Let's talk about your specific industry. What documents do you encounter? How would you preprocess and chunk them? Discuss with your neighbors.
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## Key questions to ask before embarking on RAG

    1. Is there a known structural pattern to the docs that are to be chunked? If so, leverage and take advantage of it.
    2. What are the expected queries that your users will ask? No need to be overly comprehensive, but know the key ones first.
    3. Is attribution/citation important? If so, in your text chunking, you may want to add source document metadata in your chunks as context. This is a generalizable tactic for other kinds of metadata.
    """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ## Converting Documents

    All of the texts that we dealt with above were given to us in plain text format.
    What if we had to deal with documents of varying types instead?
    How would we convert them to plain text for indexing?

    Here are a few tools you could consider using:

    1. [docling](https://docling-project.github.io/docling/) - in personal testing, this is the most turnkey open source solution.
    2. [PyMuPDF](https://github.com/pymupdf/PyMuPDF)
    3. [Tesseract](https://github.com/tesseract-ocr/tesseract)
    4. [Mistral's OCR API](https://docs.mistral.ai/capabilities/OCR/document_ai_overview/)

    There are many others available, but these are some of the easier ones to get started with.
    In particular, I am a fan of `docling` for its ease of getting started:

    ```bash
    uvx docling 2025-1091.pdf --pipeline vlm --vlm-model smoldocling --image-export-mode referenced --to doctags
    ```

    Doctags appear to be quite a useful
    """
    )
    return


@app.cell
def _():
    return


if __name__ == "__main__":
    app.run()
